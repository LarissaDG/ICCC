# Automatic Aesthetic Evaluation and Prompt Controllability in Generative Image Models  

This repository contains the code used to conduct experiments and generate results for the paper *Automatic Aesthetic Evaluation and Prompt Controllability in Generative Image Models*, submitted to ICCC 2025.  

## 📌 Overview  
The project explores automatic methods for assessing the aesthetic quality of images generated by deep learning models. Additionally, it investigates how prompt variations influence the generated output, analyzing controllability aspects in text-to-image models.  

## 📁 Repository Structure  
- `experiments/` – Scripts for running aesthetic evaluation experiments.  
- `models/` – Implementations of aesthetic scoring models.  
- `data/` – Datasets used for training and evaluation.  
- `notebooks/` – Jupyter notebooks with exploratory analyses.  
- `results/` – Output logs, metrics, and generated images.  

## ⚙️ Setup  
To set up the environment, install the required dependencies:  
```bash
pip install -r requirements.txt
```  

## 🚀 Running Experiments  
To reproduce the experiments, execute:  
```bash
python experiments/run_evaluation.py
```  

For prompt controllability analysis:  
```bash
python experiments/run_prompt_analysis.py
```  

## 📊 Results & Analysis  
The results include:  
- Aesthetic score comparisons across different models.  
- Correlations between prompt variations and generated image quality.  
- Insights into prompt engineering techniques for better controllability.  

## 📜 Citation  
If you use this code, please cite our work:  
```
@inproceedings{yourname2025iccc,
  title={Automatic Aesthetic Evaluation and Prompt Controllability in Generative Image Models},
  author={Your Name and Co-authors},
  booktitle={Proceedings of the ICCC 2025},
  year={2025}
}
```  

## 📬 Contact  
For any questions or collaborations, feel free to reach out:  
📧 your.email@example.com  

---

Caso precise de ajustes ou mais detalhes, me avise! 😊